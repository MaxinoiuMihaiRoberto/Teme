{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject 2: Text NewsGroup Classification \n",
    "1. Download the newsgroups data set using the code below. \n",
    "2. Construct a text classifier that predicts the target variable (newsgroups.target) from the input data (newsgroups.data).\n",
    "3. We will evaluate your classifier against a hold-out data set, so be sure to construct a classification function that can receive a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newsgroups.data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from text files \n",
    "Text files are actually series of words (ordered). In order to run machine learning algorithms we need to convert the text files into numerical feature vectors.\n",
    "Scikit-learn has a high level component which will create feature vectors for us ‘CountVectorizer’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts_vect = count_vect.fit_transform(newsgroups.data)\n",
    "X_train_counts_vect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing 'count_vect.fit_transform(newsgroups.data)', we are learning the vocabulary dictionary and it returns a Document-Term matrix [n_samples, n_features]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF->Term Frequencies\n",
    "\n",
    "TF-IDF-> Term Frequencies times inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "Tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = Tfidf_transformer.fit_transform(X_train_counts_vect)\n",
    "X_train_tfidf.shape\n",
    "#dimension of the Document-Term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ML algorithms\n",
    "There are various algorithms which can be used for text classification. We will start with the most simplest one Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NBML = MultinomialNB().fit(X_train_tfidf, newsgroups.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    ">>> text_Pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "...                      ('tfidf', TfidfTransformer()),\n",
    "...                      ('clf', MultinomialNB()),\n",
    "... ])\n",
    "text_Pipeline= text_Pipeline.fit(newsgroups.data, newsgroups.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of NB Classifier: Now we will test the performance of the NB classifier on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_Pipeline.predict(newsgroups_test.data)\n",
    "np.mean(predicted == newsgroups_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have newsgroups which is the trainig data set and newsgroups_test which is the testing data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM) \n",
    "Let’s try using a different algorithm SVM, and see if we can get any better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8240839086563994"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    ">>> text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "...                      ('tfidf', TfidfTransformer()),\n",
    "...                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "...                                            alpha=1e-3, random_state=42)),\n",
    "... ])\n",
    ">>> _ = text_clf_svm.fit(newsgroups.data, newsgroups.target)\n",
    ">>> predicted_svm = text_clf_svm.predict(newsgroups_test.data)\n",
    ">>> np.mean(predicted_svm == newsgroups_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "Almost all the classifiers will have various parameters which can be tuned to obtain optimal performance. Scikit gives an extremely useful tool ‘GridSearchCV’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),}\n",
    "              \n",
    "gs_clf = GridSearchCV(text_Pipeline, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(newsgroups.data, newsgroups.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=newsgroups.data,newsgroups.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Text to Numbers \n",
    "Machines, unlike humans, cannot understand the raw text. Machines can only see numbers. Particularly, statistical techniques such as machine learning can only deal with numbers. Therefore, we need to convert our text into numbers.\n",
    "\n",
    "Different approaches exist to convert text into the corresponding numerical form. The Bag of Words Model and the Word Embedding Model are two of the most commonly used approaches. In this article, we will use the bag of words model to convert our text to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency = (Number of Occurrences of a word)/(Total words in the document)\n",
    "\n",
    "IDF(word) = Log((Total number of documents)/(Number of documents containing the word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Sets \n",
    "Like any other machine learning problem, we need to split our data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Text Classification Model and Predicting Sentiment \n",
    " We will use the Random Forest Algorithm to train our model.\n",
    " To train our machine learning model using the random forest algorithm we will use RandomForestClassifier class from the sklearn.ensemble library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model \n",
    "To evaluate the performance of a classification model such as the one that we just trained, we can use metrics such as the confusion matrix, F1 measure, and the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 66   1   0   0   1   0   0   0   0   1   0   0   0   2   0  10   0   2\n",
      "    4   1]\n",
      " [  1  91   9   4   3   5   0   2   0   3   0   0   6   3   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2  89   4   0   4   0   0   0   0   0   5   4   0   5   0   0   0\n",
      "    0   0]\n",
      " [  0   4   5  85   2   2   7   2   1   3   1   0  10   2   1   2   1   0\n",
      "    0   0]\n",
      " [  0   3   3  15  81   3   3   0   1   1   0   0   5   2   2   0   0   0\n",
      "    1   0]\n",
      " [  1   7  16   1   0  85   0   1   0   0   0   0   4   0   4   1   0   0\n",
      "    0   0]\n",
      " [  0   1   2   9   0   0  83   2   0   2   0   1   2   1   0   0   0   1\n",
      "    1   0]\n",
      " [  1   1   0   2   0   2   6  90   4   0   2   2   2   2   2   0   0   0\n",
      "    1   0]\n",
      " [  0   1   0   1   2   0   3   5 111   2   0   0   0   0   0   0   1   0\n",
      "    2   0]\n",
      " [  0   1   1   0   1   1   0   0   0 104   7   1   1   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   0   1   0  10 103   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   1   2   2   0   0   0   0   0 118   1   2   1   0   0   0\n",
      "    2   0]\n",
      " [  0   6   1   6   4   1   3   7   2   3   1   0  78   3   4   0   3   0\n",
      "    0   0]\n",
      " [  0   0   0   1   1   1   0   0   0   1   0   1   4  84   3   1   1   0\n",
      "    1   0]\n",
      " [  0   2   0   1   0   3   0   1   0   0   0   0   4   1 108   1   1   0\n",
      "    0   0]\n",
      " [  2   2   0   0   1   1   0   0   0   1   0   0   1   1   0 116   0   1\n",
      "    1   0]\n",
      " [  0   0   1   0   0   0   0   2   2   2   0   0   2   0   0   0  97   1\n",
      "    7   1]\n",
      " [  1   1   0   0   0   2   0   0   0   0   0   0   0   1   1   1   1 102\n",
      "    2   0]\n",
      " [  0   1   0   0   0   0   1   0   0   2   0   1   1   1   0   1   5   4\n",
      "   63   0]\n",
      " [  5   0   0   0   0   0   0   1   0   0   0   3   0   3   0  19   3   2\n",
      "    4  35]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80        88\n",
      "           1       0.73      0.71      0.72       128\n",
      "           2       0.70      0.79      0.74       113\n",
      "           3       0.65      0.66      0.66       128\n",
      "           4       0.83      0.68      0.74       120\n",
      "           5       0.75      0.71      0.73       120\n",
      "           6       0.78      0.79      0.79       105\n",
      "           7       0.79      0.77      0.78       117\n",
      "           8       0.92      0.87      0.89       128\n",
      "           9       0.77      0.87      0.82       119\n",
      "          10       0.90      0.90      0.90       115\n",
      "          11       0.89      0.91      0.90       130\n",
      "          12       0.62      0.64      0.63       122\n",
      "          13       0.77      0.85      0.81        99\n",
      "          14       0.81      0.89      0.85       122\n",
      "          15       0.76      0.91      0.83       127\n",
      "          16       0.86      0.84      0.85       115\n",
      "          17       0.90      0.91      0.91       112\n",
      "          18       0.71      0.79      0.75        80\n",
      "          19       0.95      0.47      0.62        75\n",
      "\n",
      "    accuracy                           0.79      2263\n",
      "   macro avg       0.80      0.78      0.79      2263\n",
      "weighted avg       0.80      0.79      0.79      2263\n",
      "\n",
      "0.790543526292532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sn\n",
    "# plt.title('Confusion Matrix')\n",
    "# sn.heatmap(y_test, annot=True, vmin=0.0, vmax=100.0, fmt='.2f', cmap=cmap)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE THE MODEL\n",
    "with open('text_classifier_model', 'wb') as picklefile:\n",
    "    pickle.dump(classifier,picklefile)\n",
    "    \n",
    "# lOAD THE MODEL\n",
    "with open('text_classifier_model', 'rb') as training_model:\n",
    "    model_loaded = pickle.load(training_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sn\n",
    "# confusion_matrix = pd.crosstab(X, y_pred, rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "# sn.heatmap(confusion_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def text_classification_fct(information):\n",
    "    X, y = information.data, information.target\n",
    "    #Text Preprocessing\n",
    "    documents = []\n",
    "\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    for sen in range(0, len(X)):\n",
    "   \n",
    "        document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "       \n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "        document = document.lower()\n",
    "    \n",
    "        document = document.split()\n",
    "\n",
    "        document = [stemmer.lemmatize(word) for word in document]\n",
    "        document = ' '.join(document)\n",
    "    \n",
    "        documents.append(document)\n",
    "        \n",
    "    #Converting Text to Numbers\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "    X = vectorizer.fit_transform(documents).toarray()\n",
    "    \n",
    "    #Finding TFIDF\n",
    "    tfidfconverter = TfidfTransformer()\n",
    "    X = tfidfconverter.fit_transform(X).toarray()\n",
    "    \n",
    "    #Training and Testing Sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    #Training Text Classification Model and Predicting Sentiment\n",
    "    classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    classifier.fit(X_train.to_frame(), y_train.to_frame()) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    #Evaluating the Model\n",
    "    \n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
